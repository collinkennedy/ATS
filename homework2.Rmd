---
title: "homework2"
author: "Collin"
date: "10/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


## Question 1
- a) *Simulate a series of ğ‘›=500 Gaussian white noise observations as in Example 1.7 of textbook and compute the smaple ACF, ğœŒÌ‚(â„)to lag 20. Compare the sample ACF you obtained to the actual ACF, ğœŒ(â„)*

```{r}
set.seed(530476)
N = 500
w = rnorm(N)

plot.ts(w, main = "White Noise Process, n = 500")
acf(w, lag.max = 20, main = "White Noise Process (sample) ACF, n = 500")
#x_acf = acf(x, na.action = na.omit)
```
actual  acf:
```{r}
x = cbind(c(0:20),c(1,rep(0,20)))
plot(x[,1],x[,2], type = "h", main = "Population ACF ", xlab = "Lag", ylab = "ACF"); abline(h=0)

```
The sample ACF and Theoretical population ACF are very similar. While with the true ACF there is no autocorrelation beyond the 0th lag (because White Noise process is Independent), in the sample ACF we see there is maybe a little autocorrelation between lags, but it is not statistically significant.


- **b)**  *Repeat part (a) using only ğ‘›=50 How does changing ğ‘› affect the results?*
```{r wtf}
set.seed(530476)
N = 50
w = rnorm(N)

plot.ts(w, main = "White Noise Process, n = 50")
acf(w, lag.max = 20, main = "White Noise Process (sample) ACF, n = 50")
#x_acf = acf(x, na.action = na.omit)
```


The main difference appears to be that the second time series (with n = 50) is less cluttered. In terms of a difference seen in the sample and actual ACF plots, when n goes from 500 -> 50, the sample autocorrelation in the n = 50 case seems to be (slightly) larger (though still statistically insignificant).

## Question 2
- **a)** *Simulate a series of ğ‘›=500 moving average observations as in Example 1.8 of textbook and compute the Compare the sample ACF you obtained to the actual ACF, ğœŒ(â„*
)
when n = 500
```{r}
set.seed(530476)
N = 500
w = rnorm(N)
x = filter(w, filter=rep(1/3,3))
some_ts = x
plot(some_ts)
x_acf = acf(x, na.action = na.omit)
```

**include an image/sketch of actual ACF**
![](/Users/collinkennedy/Google Drive/UC Davis/acf.jpg)

when n = 50:
```{r}
set.seed(530476)
N = 50
w = rnorm(N)
x = filter(w, filter=rep(1/3,3))
some_ts = x
plot(some_ts)
x_acf = acf(x, na.action = na.omit)
```


## Question 3



-**a**) 
```{r}
jj = JohnsonJohnson
t = time(jj)-1970 #center it
jjReg = lm(log(JohnsonJohnson) ~ 0 + t + as.factor(cycle(JohnsonJohnson)))
summary(jjReg)

```

- **b)** *If the model is correct, what is the estimated average annual increase in the logged earnings per share?*
```{r}
sum(jjReg$coefficients)

```
The estimated average annual increase in logged earnings per share of Johnson & Johnson stock is approximately 4.33


- **c)** *If the model is correct, does the average logged earnings rate increase or decrease from the third quarter to the fourth quarter?*
```{r}
jjReg$coefficients[4]-jjReg$coefficients[5]


```
Assuming the model is correct, averaged logged earnings would decrease by .26875. This is a percentage decrease of about 23.3%.

- **d)** *What happens if you include an intercept term in the model in (a)? Explain why there was a problem.*

If you include an intercept in the model, it includes the effect of Quarter 1 earnings in each of the succeeding quarters (think of it as the baseline that following quarters are compared to). We don't want this because we want to be able to easily isolate the unique effect of each quarter on (logged) earnings per share. Including an intercept in the model also leads to the issue of perfect multicollinearity (otherwise known as dummy variable trap).

- **e** *Graph the data, ğ‘‹ğ‘¡ and superimpose the fitted values, say ğ‘‹Ì‚ğ‘¡ on the graph. Examine the residuals, ğ‘‹ğ‘¡âˆ’ğ‘‹Ì‚ğ‘¡ and state your conclusions, Does it appear that the model fits the data well (do the residuals look white)?*
```{r}
library(aTSA)
Qtr = factor(cycle(jj))
trend = time(jj) -1970
reg = lm(log(jj) ~ 0 + trend + Qtr,na.action = NULL)
plot.ts(log(jj))
lines(fitted(reg),col = 3)

```
The model appears to fit the data well (maybe almost too well? perhaps some overfitting). The residuals appear to be pretty small since the model is so closely fit to the data.


## Question 4
![](/Users/collinkennedy/Google Drive/UC Davis/UC Davis/fall_quarter_2020/STA137-Time_Series/IMG_5351.jpg)


## Question 5
![](/Users/collinkennedy/Google Drive/UC Davis/UC Davis/fall_quarter_2020/STA137-Time_Series/IMG_5352.jpg)




## Question 6
- **a** *Argue that the glacial varves series, say ğ‘‹ğ‘¡, exhibits heteroscedasticity by computing the sample variance over the first half and the second half of the data.*
```{r}
library(astsa)
#variance of the first half of the varve (glacier) time series:
var(varve[1:(length(varve))/2])


#variance of the second half
var(varve[(length(varve)/2):length(varve)])


logTransformedYt = log(varve)
ts = cbind(varve,logTransformedYt)
plot.ts(ts)

var(logTransformedYt[1:(length(logTransformedYt))/2])

var(logTransformedYt[(length(varve)/2):length(logTransformedYt)])

```
The glacier time series, Xt, appears to display heteroskedasticity, since the variance is non-constant throughout the entire series (significantly larger in the second half of the series than in the first).

However, upon performing a log-transformation on the time series, the variance appears to stabilize.
```{r}
#nontransformded
hist(varve)

#log transformed
hist(logTransformedYt)

```
The transformation does also appear to improve the normality assumption. Whereas the raw time series is heavily right skewed, once it is log transformed it looks much more normal.


- **b**  *Plot the series ğ‘Œğ‘¡ Do any time intervals, of the order 100 years, exist where one can observe behavior comparable to that observed in the global temperature
records in Figure 1.2 (page 3 of the textbook)?*
```{r}
plot.ts(logTransformedYt)

```
There does not appear to be any comparable behavior over the given time intervals between the two time series.

- **c)**  *Examine the sample ACF of ğ‘Œğ‘¡ and comment.*
```{r}
acf(logTransformedYt)

```
log transformed series (Yt) appears to be highly persistent. There is statistically significant autocorrelation between every observation and its 25+ different lags.


- **d** *Compute the difference ğ‘ˆğ‘¡=ğ‘Œğ‘¡âˆ’ğ‘Œğ‘¡âˆ’1, examine its time plot and sample ACF,
and argue that differencing the logged varve data produces a reasonably stationary
series.*
```{r}
Ut = logTransformedYt - lag(logTransformedYt,1)
plot.ts(Ut)
acf(Ut)

```
Both the time series plot and the ACF plot appear to support that notion that the times series is stationary. The time series does not appear to have any sort of time trend, as it is centered around 0 and stays that way for the duration of the series. Its variance also doesn't appear to depend on time, as it also appears to be constant. 

The ACF plot supports this. Aside from a autocorrelation at lag 1, the autocorrelations are practically 0 for the remainder of the series, which means the series' ac(v)f is not dependent on time, another indicator that this time series is indeed stationary.

